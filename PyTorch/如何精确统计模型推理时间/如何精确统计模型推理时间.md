> 本文中的知识来源为B站up主[霹雳吧啦Wz](https://space.bilibili.com/18161609)的视频[如何精确统计模型推理时间](https://www.bilibili.com/video/BV1km4SeuEim)。

# 1. 引言

下面这段代码计算模型推理用时是否正确？

```python
import time
import torchvision
import torch


if __name__ == "__main__":
    # Create a model
    mobilenetv3 = torchvision.models.mobilenet_v3_large(weights=None, num_classes=1000).eval().cuda()

    # Create a dummy input
    dummpy_input = torch.ones(size=[4, 3, 224, 224]).cuda()

    begin = time.perf_counter()

    result = mobilenetv3(dummpy_input)

    end = time.perf_counter()

    # Statistics
    print(f"The inference time: {end - begin:.4f}s")
```

```
The inference time: 0.3876s
```

在分析模型性能时需要精确地统计出模型的推理时间，但仅仅通过在模型推理前后打时间戳然后相减得到的时间其实是Host侧向Device侧下发指令的时间，如下图所示：

<a></a>
<div align=center>
    <img src=./imgs_markdown/2024-09-24-15-45-44.png
    width=70%></br><center></center>
</div>

其中：

- Host侧是CPU
- 而Device侧是GPU。

需要注意的是：Host与Device其实异步的，意思是CPU下发完指令之后就结束了，不会等待GPU是否完成；GPU也是一样的，并不会等CPU。因此模型理论上模型的推理时间应该为：

$$
模型实际推理时间 = \mathrm{Host下发指令总时间} \cup \mathrm{Device总时间}
$$

所以如上图的左侧，因为Host侧的时间比Device侧的时间短，所以上面的代码是不行的；对于上图的右侧，因为Host侧的时间比Device侧的时间长，所以上面的代码是可以的。

# 2. 两种常见的统计方式

🤔 𝑸𝒖𝒆𝒔𝒕𝒊𝒐𝒏：我测，这样也太麻烦了，难道我还需要把握Host侧和Device侧的时间吗？
🥳 𝑨𝒏𝒔𝒘𝒆𝒓：PyTorch官方也考虑到了这个问题，因此给出了两个常见的统计方法：

1. 通过手动调用同步函数确保Device计算完成（Host侧需要等待Device侧）
2. 通过Event方法统计时间

<a></a>
<div align=center>
    <img src=./imgs_markdown/2024-09-24-15-57-29.png
    width=70%></br><center></center>
</div>

## 2.1 方法1：通过手动调用同步函数确保Device计算完成

代码示例如下：

```python
torch.cuda.synchronize(device)  # Host侧等待Device侧计算完成
begin = time.perf_counter()  # Host侧
results = model(x)  # Host侧+Device侧
torch.cuda.synchronize(device)  # Host侧等待Device侧计算完成
end = time.perf_counter()  # Host侧
```

过程如下图所示：

<a></a>
<div align=center>
    <img src=./imgs_markdown/2024-09-24-16-03-37.png
    width=60%></br><center></center>
</div>

## 2.2 方法2：通过Event方法统计时间

代码示例如下：

```python
# 创建两个Event对象，用于记录和测量CUDA操作的时间
start_event = torch.cuda.Event(enable_timing=True)  # 开启计时功能
end_event = torch.cuda.Event(enable_timing=True)    # 开启计时功能

# 记录当前时间点，作为后续计算时间差的起始点
start_event.record()

results = model(x)

# 记录当前时间点，作为后续计算时间差的结束点
end_event.record()

# 确保所有CUDA操作完成，这样测量的时间才准确
end_event.synchronize()

# 计算从start_event到end_event的时间差，单位是毫秒
elapsed_time = start_event.elapsed_time(end_event)
```

过程如下图所示：

<a></a>
<div align=center>
    <img src=./imgs_markdown/2024-09-24-16-03-47.png
    width=60%></br><center></center>
</div>

# 3. 代码实测

下面示例代码中分别给出了三种方式的结果，每种方法都重复50次，忽略前5次推理，取后45次的平均值。

1. 直接打时间戳计算
2. 手动调用同步函数
3. 使用Event方法

```python
import time
import torchvision
import torch
from tqdm import tqdm as TQDM
import numpy as np


def calc_inference_time_method_1(model: torch.nn.Module, input: torch.Tensor, times: int=50) -> float:
    with torch.inference_mode():
        time_list: list = []

        for _ in TQDM(range(times), desc='Method 1'):
            begin = time.perf_counter()
            results = model(input)
            end = time.perf_counter()
            time_list.append(end - begin)
            
    return np.average(time_list[5: ])


def calc_inference_time_method_2(model: torch.nn.Module, input: torch.Tensor, times: int=50) -> float:
    device = input.device
    with torch.inference_mode():
        time_list: list = []

        for _ in TQDM(range(times), desc='Method 2'):
            torch.cuda.synchronize(device=device)
            begin = time.perf_counter()
            results = model(input)
            torch.cuda.synchronize(device=device)
            end = time.perf_counter()
            time_list.append(end - begin)

    return np.average(time_list[5: ])


def calc_inference_time_method_3(model: torch.nn.Module, input: torch.Tensor, times: int=50) -> float:
    with torch.inference_mode():
        start_event = torch.cuda.Event(enable_timing=True)
        end_event = torch.cuda.Event(enable_timing=True)
        time_list: list = []

        for _ in TQDM(range(times), desc='Method 3'):
            start_event.record()
            results = model(input)
            end_event.record()
            end_event.synchronize()
            time_list.append(start_event.elapsed_time(end_event) / 1000)

    return np.average(time_list[5: ])


if __name__ == "__main__":
    # Create a model
    mobilenetv3 = torchvision.models.mobilenet_v3_large(weights=None, num_classes=10, width_mult=2.0).cuda()

    # Create a dummy input
    dummpy_input = torch.randn(size=[32, 3, 224, 224]).cuda()

    time_1 = calc_inference_time_method_1(model=mobilenetv3, input=dummpy_input, times=50)
    print(f"The inference time of method 1: {time_1:.4f}s")

    time_2 = calc_inference_time_method_2(model=mobilenetv3, input=dummpy_input, times=50)
    print(f"The inference time of method 2: {time_2:.4f}s")

    time_3 = calc_inference_time_method_3(model=mobilenetv3, input=dummpy_input, times=50)
    print(f"The inference time of method 3: {time_3:.4f}s")
```

```
The inference time of method 1: 0.0269s
The inference time of method 2: 0.0331s
The inference time of method 3: 0.0324s
```

通过终端输出可以看到，如果直接在模型推理前后打时间戳相减得到的时间会短一些（因为并没有等待Device侧计算完成）。而使用同步函数（`torch.cuda.synchronize()`）或者`Event`方法统计的时间明显要长很多。

# 4. 三种方法对比

通过上面的代码示例可以看到，通过同步函数（`torch.cuda.synchronize()`）统计的时间和`Event`方法统计的时间基本一致。那两者有什么区别呢？如果只是简单统计一个模型的推理时间确实看不出什么差异。但如果要统计一个完整AI应用pipline（其中可能包含多个模型以及各种CPU计算）中不同模型的耗时，而又不想影响到整个pipline的性能，那么建议使用`Event`方法。因为使用同步函数可能会让Host长期处于等待状态，等待过程中也无法干其他的事情，从而导致计算资源的浪费。可以看看下面这个示例，整个pipline由MobileNetV3-Large推理+一段纯CPU计算+MobileNetV3-Small推理串行构成，假设想统计一下MobileNetV3-Large、MobileNetV3-Small推理分别用了多长时间：

```python
import time
import torchvision
import torch
from tqdm import tqdm as TQDM
import numpy as np


def cpu_task() -> None:
    x = np.random.randn(1, 3, 512, 512)
    x = x.astype(np.float32)
    x = x * 1024 ** 0.5


def calc_inference_time_method_1(model_1: torch.nn.Module, model_2: torch.nn.Module, input: torch.Tensor, times: int=50) -> tuple:
    with torch.inference_mode():
        time_list_1: list = []
        time_list_2: list = []

        for _ in TQDM(range(times), desc='Method 1'):
            # step.1: model 1
            begin_1 = time.perf_counter()
            results_1 = model_1(input)
            end_1 = time.perf_counter()

            # step.2: CPU
            cpu_task()

            # step.3: model 2
            begin_2 = time.perf_counter()
            results_2 = model_2(input)
            end_2 = time.perf_counter()

            time_list_1.append(end_1 - begin_1)
            time_list_2.append(end_2 - begin_2)
            
    return np.average(time_list_1[5: ]), np.average(time_list_2[5: ])


def calc_inference_time_method_2(model_1: torch.nn.Module, model_2: torch.nn.Module, input: torch.Tensor, times: int=50) -> tuple:
    device = input.device
    with torch.inference_mode():
        time_list_1: list = []
        time_list_2: list = []

        for _ in TQDM(range(times), desc='Method 2'):
            # step.1: model 1
            torch.cuda.synchronize(device=device)
            begin_1 = time.perf_counter()
            results_1 = model_1(input)
            torch.cuda.synchronize(device=device)
            end_1 = time.perf_counter()

            # step.2: CPU
            cpu_task()

            # step.3: model 2
            torch.cuda.synchronize(device=device)
            begin_2 = time.perf_counter()
            results_2 = model_2(input)
            torch.cuda.synchronize(device=device)
            end_2 = time.perf_counter()

            time_list_1.append(end_1 - begin_1)
            time_list_2.append(end_2 - begin_2)

    return np.average(time_list_1[5: ]), np.average(time_list_2[5: ])


def calc_inference_time_method_3(model_1: torch.nn.Module, model_2: torch.nn.Module, input: torch.Tensor, times: int=50) -> tuple:
    with torch.inference_mode():
        start_event_1 = torch.cuda.Event(enable_timing=True)
        end_event_1 = torch.cuda.Event(enable_timing=True)
        time_list_1: list = []

        start_event_2 = torch.cuda.Event(enable_timing=True)
        end_event_2 = torch.cuda.Event(enable_timing=True)
        time_list_2: list = []

        for _ in TQDM(range(times), desc='Method 3'):
            # Step.1 model 1
            start_event_1.record()
            results_1 = model_1(input)
            end_event_1.record()
            end_event_1.synchronize()

            # Step.2 CPU
            cpu_task()

            # Step.3 model 2
            start_event_2.record()
            results_2 = model_1(input)
            end_event_2.record()
            end_event_2.synchronize()

            time_list_1.append(start_event_1.elapsed_time(end_event_1) / 1000)
            time_list_2.append(start_event_2.elapsed_time(end_event_2) / 1000)

    return np.average(time_list_1[5: ]), np.average(time_list_2[5: ])


if __name__ == "__main__":
    # Create a model
    mobilenetv3_large = torchvision.models.mobilenet_v3_large(weights=None, num_classes=1000, width_mult=1.0).cuda()
    mobilenetv3_small = torchvision.models.mobilenet_v3_small(weights=None, num_classes=1000, width_mult=1.0).cuda()

    # Create a dummy input
    dummpy_input = torch.randn(size=[32, 3, 224, 224]).cuda()

    method_1_time_1, method_1_time_2 = calc_inference_time_method_1(model_1=mobilenetv3_large, model_2=mobilenetv3_small, input=dummpy_input, times=50)
    print(
        f"[Method 1]\n"
        f"\tThe inference time of model 1: {method_1_time_1:.4f}s\n"
        f"\tThe inference time of model 2: {method_1_time_2:.4f}s\n"
        f"\tThe average time: {np.average([method_1_time_1, method_1_time_2]):.4f}s\n"
    )

    method_2_time_1, method_2_time_2 = calc_inference_time_method_2(model_1=mobilenetv3_large, model_2=mobilenetv3_small, input=dummpy_input, times=50)
    print(
        f"[Method 2]\n"
        f"\tThe inference time of model 1: {method_2_time_1:.4f}s\n"
        f"\tThe inference time of model 2: {method_2_time_2:.4f}s\n"
        f"\tThe average time: {np.average([method_2_time_1, method_1_time_2]):.4f}s\n"
    )

    method_3_time_1, method_3_time_2 = calc_inference_time_method_3(model_1=mobilenetv3_large, model_2=mobilenetv3_small, input=dummpy_input, times=50)
    print(
        f"[Method 3]\n"
        f"\tThe inference time of model 1: {method_3_time_1:.4f}s\n"
        f"\tThe inference time of model 2: {method_3_time_2:.4f}s\n"
        f"\tThe average time: {np.average([method_3_time_1, method_1_time_2]):.4f}s\n"
    )
```

```
[Method 1]
        The inference time of model 1: 0.0037s
        The inference time of model 2: 0.0031s
        The average time: 0.0034s

[Method 2]
        The inference time of model 1: 0.0134s
        The inference time of model 2: 0.0044s
        The average time: 0.0082s

[Method 3]
        The inference time of model 1: 0.0129s
        The inference time of model 2: 0.0128s
        The average time: 0.0080s
```

通过终端打印的结果可以看到无论是使用同步函数还是Event方法统计的MobileNetV3-Large、MobileNetV3-Small的推理时间基本是一致的。但对于整个pipline而言使用同步函数时总时间明显变长了。下图大致解释了为什么使用同步函数时导致整个pipline变长的原因，主要是在MobileNetV3-Large发送完指令后使用同步函数时会一直等待Device侧计算结束，期间啥也不能干。而使用Event方法时在MobileNetV3-Large发送完指令后不会阻塞Host，可以立马去进行后面的CPU计算任务。具体的时序图如下所示：

<a></a>
<div align=center>
    <img src=./imgs_markdown/2024-09-24-17-28-42.png
    width=100%></br><center></center>
</div>
